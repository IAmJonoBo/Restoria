{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment detection and summary (safe for local/Colab)\n",
    "import os, sys, platform, json, datetime\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "CUDA = False\n",
    "MPS = False\n",
    "TPU = False\n",
    "try:\n",
    "    import torch\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    MPS = getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm  # noqa\n",
    "    TPU = True\n",
    "except Exception:\n",
    "    pass\n",
    "print(json.dumps({\n",
    "    'in_colab': bool(IN_COLAB),\n",
    "    'cuda': bool(CUDA),\n",
    "    'mps': bool(MPS),\n",
    "    'tpu': bool(TPU),\n",
    "    'python': sys.version.split()[0],\n",
    "    'platform': platform.platform(),\n",
    "    'timestamp': datetime.datetime.utcnow().isoformat()+'Z'\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3749a",
   "metadata": {},
   "source": [
    "# Restoria Colab Notebook\n",
    "\n",
    "A Colab-optimized walkthrough for using the Restoria toolkit (GFPGAN-compatible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports used throughout the notebook\n",
    "from PIL import Image  # noqa: F401\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFPGAN Colab Demo (Fork)\n",
    "\n",
    "This Colab notebook installs dependencies, clones this fork, downloads pretrained weights automatically, and runs the GFPGAN inference script on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d9709",
   "metadata": {
    "id": "env-setup"
   },
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run(cmd, shell=False, check=True):\n",
    "    print(\">>\", cmd if shell else \" \".join(cmd))\n",
    "    if shell:\n",
    "        return subprocess.run(cmd, shell=True, check=check)\n",
    "    else:\n",
    "        return subprocess.run(cmd, check=check)\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "# CI smoke: robust env var parsing (treat '0'/'false' as False)\n",
    "_ci_val = os.environ.get(\"NB_CI_SMOKE\", \"\").strip().lower()\n",
    "CI_SMOKE = _ci_val in (\"1\", \"true\", \"yes\", \"y\")\n",
    "if CI_SMOKE:\n",
    "    print(\"NB_CI_SMOKE=1: skipping dependency install step\")\n",
    "\n",
    "if not CI_SMOKE:\n",
    "    # Upgrade pip tooling quietly\n",
    "    run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--quiet\", \"pip\", \"setuptools\", \"wheel\"])\n",
    "    # Ensure IPython deps are satisfied to avoid resolver warnings\n",
    "    run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"-U\", \"ipython\", \"jedi\"])\n",
    "\n",
    "    # Install torch pinned to repo-compatible versions\n",
    "    has_nvidia = shutil.which(\"nvidia-smi\") is not None\n",
    "    torch_spec = \"torch==2.1.*\"\n",
    "    vision_spec = \"torchvision==0.16.*\"\n",
    "    audio_spec = \"torchaudio==2.1.*\"\n",
    "    if has_nvidia:\n",
    "        print(\"Detected NVIDIA GPU; installing CUDA wheels (cu121)\")\n",
    "        run([\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--quiet\",\n",
    "            torch_spec,\n",
    "            vision_spec,\n",
    "            audio_spec,\n",
    "            \"--index-url\",\n",
    "            \"https://download.pytorch.org/whl/cu121\",\n",
    "        ])\n",
    "    else:\n",
    "        print(\"No NVIDIA GPU; installing CPU wheels\")\n",
    "        run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", torch_spec, vision_spec, audio_spec])\n",
    "\n",
    "    # Core deps: match repo constraints (basicsr <= 1.4.2)\n",
    "    run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--quiet\",\n",
    "            \"--no-cache-dir\",\n",
    "            \"--upgrade\",\n",
    "            \"basicsr<=1.4.2\",\n",
    "            \"facexlib\",\n",
    "            \"realesrgan\",\n",
    "            \"opencv-python\",\n",
    "            \"tqdm\",\n",
    "            \"numpy\",\n",
    "            \"PyYAML\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56a79c",
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "REPO_URL = \"https://github.com/IAmJonoBo/Restoria.git\"\n",
    "REPO_DIR = \"GFPGAN\"  # Keep folder name for compatibility with notebook paths\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"Removing existing {REPO_DIR} directory...\")\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "print(f\"Cloning repository from {REPO_URL}...\")\n",
    "subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
    "print(\"Clone complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bced545",
   "metadata": {
    "id": "run-inference"
   },
   "outputs": [],
   "source": [
    "#@title Run inference on sample images\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# CI smoke: skip heavy inference when NB_CI_SMOKE is set\n",
    "if os.environ.get(\"NB_CI_SMOKE\"):\n",
    "    print(\"NB_CI_SMOKE=1: skipping sample inference step\")\n",
    "else:\n",
    "    repo_dir = \"GFPGAN\"\n",
    "    cmd = [sys.executable, \"inference_gfpgan.py\", \"-i\", \"inputs/whole_imgs\", \"-o\", \"results\", \"-v\", \"1.4\", \"-s\", \"2\"]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    res = subprocess.run(cmd, cwd=repo_dir, check=False)\n",
    "    print(\"Exit code:\", res.returncode)\n",
    "    print(\"Results written to ./GFPGAN/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b7758",
   "metadata": {
    "id": "display-results"
   },
   "outputs": [],
   "source": [
    "# @title Display a few restored images\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# CI smoke: do not try to display images in headless CI\n",
    "if os.environ.get(\"NB_CI_SMOKE\"):\n",
    "    print(\"NB_CI_SMOKE=1: skipping image display\")\n",
    "else:\n",
    "    base = \"GFPGAN/results/restored_imgs\"\n",
    "    imgs = sorted(glob.glob(os.path.join(base, \"*\")))[:4]\n",
    "    print(f\"Displaying {len(imgs)} images from\", base)\n",
    "    for p in imgs:\n",
    "        try:\n",
    "            display(Image(filename=p))\n",
    "        except Exception:\n",
    "            print(\"Could not display:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b78da",
   "metadata": {
    "id": "upload-run"
   },
   "outputs": [],
   "source": [
    "# @title Upload your own images and run GFPGAN\n",
    "# @markdown Upload one or more images; they will be processed with the settings below.\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Guard: if not in Colab, skip gracefully; also skip in CI smoke\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if os.environ.get(\"NB_CI_SMOKE\"):\n",
    "    print(\"NB_CI_SMOKE=1: skipping upload/inference UI\")\n",
    "else:\n",
    "    upload_dir = \"GFPGAN/uploads\"\n",
    "    os.makedirs(upload_dir, exist_ok=True)\n",
    "\n",
    "    if IN_COLAB:\n",
    "        uploaded = files.upload()\n",
    "        for name, data in uploaded.items():\n",
    "            with open(name, \"wb\") as f:\n",
    "                f.write(data)\n",
    "            shutil.move(name, os.path.join(upload_dir, os.path.basename(name)))\n",
    "    else:\n",
    "        print(\"Not running in Colab; skipping upload UI.\")\n",
    "\n",
    "    # Inference parameters\n",
    "    version = \"1.4\"  # @param ['1', '1.2', '1.3', '1.4']\n",
    "    scale = 2  # @param {type: 'integer'}\n",
    "    only_center_face = False  # @param {type: 'boolean'}\n",
    "    aligned = False  # @param {type: 'boolean'}\n",
    "    autopilot = False  # @param {type: 'boolean'}\n",
    "    hardware_aware = True  # @param {type: 'boolean'}\n",
    "    select_by = \"sharpness\"  # @param ['sharpness', 'identity']\n",
    "\n",
    "    print(\"Running inference on uploads...\")\n",
    "    cmd = [sys.executable, \"inference_gfpgan.py\", \"-i\", \"uploads\", \"-o\", \"results\", \"-v\", str(version), \"-s\", str(scale)]\n",
    "    if only_center_face:\n",
    "        cmd.append(\"--only_center_face\")\n",
    "    if aligned:\n",
    "        cmd.append(\"--aligned\")\n",
    "    if autopilot:\n",
    "        cmd.append(\"--auto\")\n",
    "        cmd.extend([\"--select-by\", select_by])\n",
    "    if hardware_aware:\n",
    "        cmd.append(\"--auto-hw\")\n",
    "    subprocess.run(cmd, cwd=\"GFPGAN\", check=False)\n",
    "    print(\"Done. See GFPGAN/results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- GPU acceleration is optional; uncomment the CUDA wheel index in the install cell to use GPU on Colab.\n",
    "- The inference script will automatically download the GFPGAN v1.4 weights if not present.\n",
    "- For large batches, consider enabling the Real-ESRGAN background upsampler by keeping default settings (it is auto-disabled on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59853cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Dependencies (Colab-friendly; quiet). Skipped if not in Colab to avoid local env churn.\n",
    "import subprocess\n",
    "if 'google.colab' in sys.modules:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-U',\n",
    "                    'huggingface_hub>=0.23',\n",
    "                    'Pillow>=9'], check=False)\n",
    "print('Deps ready (or skipped locally).')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cea25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Determinism: seed and flags for reproducibility\n",
    "import random\n",
    "try:\n",
    "    import numpy as np\n",
    "except Exception:\n",
    "    np = None\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "SEED = 123\n",
    "DETERMINISTIC = True\n",
    "random.seed(SEED)\n",
    "if np is not None:\n",
    "    np.random.seed(SEED)\n",
    "if torch is not None:\n",
    "    torch.manual_seed(SEED)\n",
    "    if DETERMINISTIC:\n",
    "        try:\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "        except Exception:\n",
    "            pass\n",
    "print({'seed': SEED, 'deterministic': DETERMINISTIC})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3953ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Quickstart: run Restoria in dry-run mode on a sample image\n",
    "import os, sys, subprocess, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure in-repo import works if not installed\n",
    "if 'restoria' not in sys.modules:\n",
    "    sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "# Find a small sample image\n",
    "candidates = [\n",
    "    'assets/gfpgan_logo.png',\n",
    "    'bench/samples/portrait.jpg',\n",
    "]\n",
    "img = None\n",
    "for c in candidates:\n",
    "    if os.path.exists(c):\n",
    "        img = c\n",
    "        break\n",
    "if img is None:\n",
    "    os.makedirs('tmp', exist_ok=True)\n",
    "    img = 'tmp/blank.png'\n",
    "    open(img,'wb').write(b'\\x89PNG\\r\\n\\x1a\\n')\n",
    "\n",
    "out_dir = 'results/dry_demo/notebook_quickstart'\n",
    "# Use the Python entrypoint to avoid shell specifics in Colab\n",
    "from restoria.cli.main import main as restoria_main\n",
    "rc = restoria_main(['run', '--input', img, '--output', out_dir, '--dry-run', '--device', 'auto'])\n",
    "print('Return code:', rc)\n",
    "print('Wrote:', out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144968ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Outputs summary: show manifest and metrics (short)\n",
    "import json, os\n",
    "out_dir = 'results/dry_demo/notebook_quickstart'\n",
    "manifest_p = os.path.join(out_dir, 'manifest.json')\n",
    "metrics_p = os.path.join(out_dir, 'metrics.json')\n",
    "if os.path.exists(manifest_p):\n",
    "    m = json.load(open(manifest_p))\n",
    "    print('device:', m.get('device'))\n",
    "    print('args.device:', m.get('args',{}).get('device'))\n",
    "if os.path.exists(metrics_p):\n",
    "    mm = json.load(open(metrics_p))\n",
    "    recs = mm.get('metrics', [])\n",
    "    print('num_records:', len(recs))\n",
    "    if recs:\n",
    "        print('first_record_keys:', sorted(recs[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e1afe",
   "metadata": {},
   "source": [
    "### Real run (optional)\n",
    "\n",
    "To perform a full restoration run (non-dry), switch `--dry-run` off. This may download model weights on first use and take longer:\n",
    "\n",
    "```python\n",
    "from restoria.cli.main import main as restoria_main\n",
    "img = 'assets/gfpgan_logo.png'\n",
    "out_dir = 'results/real_demo/notebook_quickstart'\n",
    "restoria_main(['run', '--input', img, '--output', out_dir, '--device', 'auto', '--metrics', 'fast'])\n",
    "```\n",
    "\n",
    "Tips:\n",
    "- Use `--seed` and `--deterministic` for reproducibility\n",
    "- Try `--compile` for potential speed-ups (falls back safely if unsupported)\n",
    "- ONNX Runtime path requires the ORT extras installed; otherwise itâ€™s skipped gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Optional: metrics demo (fast). Skip in CI to avoid heavy deps/weights.\n",
    "import os, sys\n",
    "if os.getenv('CI','0') != '1':\n",
    "    from pathlib import Path\n",
    "    if 'restoria' not in sys.modules:\n",
    "        sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "    from restoria.cli.main import main as restoria_main\n",
    "    img = 'assets/gfpgan_logo.png' if os.path.exists('assets/gfpgan_logo.png') else 'tmp/blank.png'\n",
    "    out_dir = 'results/metrics_demo'\n",
    "    rc = restoria_main(['run','--input', img,'--output', out_dir,'--device','auto','--metrics','fast'])\n",
    "    print('metrics demo rc:', rc)\n",
    "    mp = os.path.join(out_dir,'metrics.json')\n",
    "    if os.path.exists(mp):\n",
    "        import json\n",
    "        data = json.load(open(mp))\n",
    "        ms = data.get('metrics', [])\n",
    "        if ms and isinstance(ms, list):\n",
    "            print('arcface_cosine:', ms[0].get('metrics',{}).get('arcface_cosine'))\n",
    "else:\n",
    "    print('CI detected; skipping metrics demo.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
